{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master b88ed09] working\n",
      " 1 file changed, 526 insertions(+), 878 deletions(-)\n",
      " rewrite OpenDataRussia/OrthodoxChurh/Parser.ipynb (80%)\n",
      "Counting objects: 5, done.\n",
      "Delta compression using up to 4 threads.\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (5/5), 6.51 KiB | 0 bytes/s, done.\n",
      "Total 5 (delta 1), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To https://github.com/DmitrySerg/OpenData\n",
      "   c4dcdf5..b88ed09  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git add Parser.ipynb\n",
    "!git commit -m \"working\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Цель\n",
    "- Собрать базу открытых данных с сайта Московской патриархии о персоналиях, организациях, богослужебных указаниях и храмах, структурировать их и загрузить в хаб открытых данных\n",
    "\n",
    "## Задача\n",
    "- проинвентаризировать данные на сайте Московской патриархии\n",
    "- написать парсеры и выгрузить данные в форматах CSV и JSON\n",
    "- загрузить данные в хаб открытых данных http://hubofdata.ru в удобном виде.\n",
    "## Требования\n",
    "- открытый исходный код в Github под свободной лицензией\n",
    "- межплатформенный код (возможность запуска на Linux/Windows)\n",
    "- предусмотреть перенос всех метаданных в CKAN в виде тегов и атрибутов\n",
    "- переносить все данные в CKAN или на отдельный хостинг (например, в Github)\n",
    "## Данные\n",
    "- Персоналии - http://www.patriarchia.ru/db/persons/\n",
    "- Организации - http://www.patriarchia.ru/db/organizations/\n",
    "- Богослужебные указания - http://www.patriarchia.ru/bu/\n",
    "- Храмы - http://map.patriarhia.ru/\n",
    "## Пожелания\n",
    "- реализация в виде программы командной строки или веб-приложения\n",
    "- возможность запуска на MacOSX\n",
    "- использовать скриптовые языки такие как Python, Perl, R и другие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests      \n",
    "import numpy as np   \n",
    "import pandas as pd  \n",
    "import time          \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_link = \"http://www.patriarchia.ru\"\n",
    "link_persons = \"http://www.patriarchia.ru/db/persons/\"\n",
    "link_organizations = \"http://www.patriarchia.ru/db/organizations/\"\n",
    "link_methodology = \"http://www.patriarchia.ru/bu/\"\n",
    "link_churches = \"http://map.patriarhia.ru/\"\n",
    "\n",
    "file_path_persons = os.getcwd() + \"/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "code_folding": [
     5,
     34,
     79
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Persons:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def getSoup(self, link):\n",
    "        \"\"\"\n",
    "            Returns bs4-formatted page\n",
    "        \"\"\"\n",
    "        html = requests.get(link)\n",
    "        if html.status_code == 200:\n",
    "            soup = BeautifulSoup(html.content, 'lxml')\n",
    "        else:\n",
    "            soup = None\n",
    "        return soup\n",
    "    \n",
    "    def getPersonLinks(self):\n",
    "        \"\"\"\n",
    "            Returns a list with names, links and affiliations    \n",
    "        \"\"\"\n",
    "\n",
    "        persons = getSoup(link_persons)\n",
    "        persons = persons.find('div', attrs={\"class\":\"main\"})\n",
    "        persons = persons.findAll('div', attrs={'class':'news'})\n",
    "\n",
    "        dfPersons = []\n",
    "        for person in persons:\n",
    "            name = person.find('h4').text\n",
    "            link = main_link + person.find('a')['href']\n",
    "            affiliation = person.find('div').text\n",
    "            dfPersons.append({\"name\":name, \"link\":link, \"affiliation\":affiliation})\n",
    "        \n",
    "        return dfPersons\n",
    "\n",
    "    def getPersonData(self, link):\n",
    "        \"\"\"\n",
    "            Returns a dict with parsed field:value pairs from personal page\n",
    "\n",
    "            link: string\n",
    "                link to personal page\n",
    "        \"\"\"\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        # first - get the page content\n",
    "        pageContent = getSoup(link)\n",
    "\n",
    "        try:\n",
    "            image_link = pageContent.find('div', attrs={'class':'news_img'})\n",
    "            image_link = image_link.find('img')['src']\n",
    "            data.update({'image_link':image_link})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # now concentrate on text\n",
    "        pageContent = pageContent.find('div', attrs={'class':'main'})\n",
    "        pageContent = pageContent.findAll(\"dl\")\n",
    "\n",
    "        # second - find all field-value pairs on page\n",
    "        for text in pageContent:        \n",
    "            number_of_fields = len(text.findAll('dt'))*2-1\n",
    "            if number_of_fields>0:\n",
    "                parsedFields = []\n",
    "                currentValue = text.find('dt')\n",
    "                parsedFields.append(currentValue.getText().strip())\n",
    "\n",
    "                while number_of_fields>0:\n",
    "                    currentValue = currentValue.nextSibling\n",
    "                    if currentValue != '\\n':\n",
    "                        value = currentValue.getText().strip()\n",
    "                        if value:\n",
    "                            parsedFields.append(value)\n",
    "                            number_of_fields -= 1\n",
    "\n",
    "                text = dict(zip(parsedFields[::2], parsedFields[1::2]))\n",
    "                data.update(text)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def saveDataFrame(self, final):\n",
    "        \"\"\"\n",
    "            Save final file into csv and json files\n",
    "        \"\"\"\n",
    "        self.df = pd.DataFrame(final)\n",
    "        self.df.columns = self.df.columns.str.strip(':')\n",
    "        self.df = self.df[['name','affiliation','Страна','Дата рождения', \n",
    "                           'Дата пострига','Дата хиротонии', 'Дата смерти',\n",
    "                           'День ангела', 'Епархия', 'Место работы', 'Образование', \n",
    "                           'Биография', 'Награды', 'Научные труды, публикации', \n",
    "                           'E-mail', 'Web-сайт', 'link', 'image_link']]\n",
    "        self.df.to_csv(file_path_persons+'persons.csv', encoding='utf8')\n",
    "        seff.df.to_json(file_path_persons+'persons.json')\n",
    "        \n",
    "    \n",
    "    def runParsing(self):\n",
    "        self.dfPersons = getPersonLinks()\n",
    "        final = []\n",
    "        for person in tqdm_notebook(self.dfPersons):\n",
    "            try:\n",
    "                data = getPersonData(person['link'])\n",
    "                person.update(data)\n",
    "                final.append(person)\n",
    "                time.sleep(0.7)\n",
    "            except Exception as e:\n",
    "                # print(e)                           # DEBUG\n",
    "                # print('problem',  person['link'])  # DEBUG\n",
    "                time.sleep(30)\n",
    "                \n",
    "        self.saveDataFrame(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "per = Persons()\n",
    "per.runParsing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSoup(link):\n",
    "    \"\"\"\n",
    "        Returns bs4-formatted page\n",
    "    \"\"\"\n",
    "    html = requests.get(link)\n",
    "    if html.status_code == 200:\n",
    "        soup = BeautifulSoup(html.content, 'lxml')\n",
    "    else:\n",
    "        soup = None\n",
    "    return soup\n",
    "\n",
    "def getPersonLinks():\n",
    "    \"\"\"\n",
    "        Returns a list with names, links and affiliations    \n",
    "    \"\"\"\n",
    "    \n",
    "    persons = getSoup(link_persons)\n",
    "    persons = persons.find('div', attrs={\"class\":\"main\"})\n",
    "    persons = persons.findAll('div', attrs={'class':'news'})\n",
    "    \n",
    "    dfPersons = []\n",
    "    for person in persons:\n",
    "        name = person.find('h4').text\n",
    "        link = main_link + person.find('a')['href']\n",
    "        affiliation = person.find('div').text\n",
    "        dfPersons.append({\"name\":name, \"link\":link, \"affiliation\":affiliation})\n",
    "        \n",
    "    return dfPersons\n",
    "\n",
    "def getPersonData(link):\n",
    "    \"\"\"\n",
    "        Returns a dict with parsed field:value pairs from personal page\n",
    "    \n",
    "        link: string\n",
    "            link to personal page\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # first - get the page content\n",
    "    pageContent = getSoup(link)\n",
    "    \n",
    "    try:\n",
    "        image_link = pageContent.find('div', attrs={'class':'news_img'})\n",
    "        image_link = image_link.find('img')['src']\n",
    "        data.update({'image_link':image_link})\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # now concentrate on text\n",
    "    pageContent = pageContent.find('div', attrs={'class':'main'})\n",
    "    pageContent = pageContent.findAll(\"dl\")\n",
    "    \n",
    "    # second - find all field-value pairs on page\n",
    "    for text in pageContent:        \n",
    "        number_of_fields = len(text.findAll('dt'))*2-1\n",
    "        if number_of_fields>0:\n",
    "            parsedFields = []\n",
    "            currentValue = text.find('dt')\n",
    "            parsedFields.append(currentValue.getText().strip())\n",
    "\n",
    "            while number_of_fields>0:\n",
    "                currentValue = currentValue.nextSibling\n",
    "                if currentValue != '\\n':\n",
    "                    value = currentValue.getText().strip()\n",
    "                    if value:\n",
    "                        parsedFields.append(value)\n",
    "                        number_of_fields -= 1\n",
    "\n",
    "            text = dict(zip(parsedFields[::2], parsedFields[1::2]))\n",
    "            data.update(text)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllLinks(url):\n",
    "    \"\"\"\n",
    "        Returns a list with names, links and affiliations    \n",
    "        \n",
    "        url: string\n",
    "            url to parse data from\n",
    "    \"\"\"\n",
    "    \n",
    "    links = getSoup(url)\n",
    "    links = links.find('div', attrs={\"class\":\"main\"})\n",
    "    links = links.findAll('div', attrs={'class':'news'})\n",
    "    \n",
    "    dfLinks = []\n",
    "    for link in links:\n",
    "        name = link.find('h4').text\n",
    "        linkParsed = main_link + link.find('a')['href']\n",
    "        affiliation = link.find('div').text\n",
    "        dfLinks.append({\"name\":name, \"link\":linkParsed, \"affiliation\":affiliation})\n",
    "        \n",
    "    return dfLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfOrganizations = getAllLinks(link_organizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affiliation': 'Епархии',\n",
       " 'link': 'http://www.patriarchia.ru/db/text/31086.html',\n",
       " 'name': 'Абаканская епархия'}"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOrganizations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
