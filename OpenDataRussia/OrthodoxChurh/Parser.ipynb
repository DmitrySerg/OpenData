{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 5560542] added unified approach for persons and organizations\n",
      " 1 file changed, 343 insertions(+), 72 deletions(-)\n",
      "Counting objects: 5, done.\n",
      "Delta compression using up to 4 threads.\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (5/5), 3.20 KiB | 0 bytes/s, done.\n",
      "Total 5 (delta 2), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/DmitrySerg/OpenData\n",
      "   ff136af..5560542  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git add Parser.ipynb\n",
    "!git commit -m \"added unified approach for persons and organizations\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Цель\n",
    "- Собрать базу открытых данных с сайта Московской патриархии о персоналиях, организациях, богослужебных указаниях и храмах, структурировать их и загрузить в хаб открытых данных\n",
    "\n",
    "## Задача\n",
    "- проинвентаризировать данные на сайте Московской патриархии\n",
    "- написать парсеры и выгрузить данные в форматах CSV и JSON\n",
    "- загрузить данные в хаб открытых данных http://hubofdata.ru в удобном виде.\n",
    "## Требования\n",
    "- открытый исходный код в Github под свободной лицензией\n",
    "- межплатформенный код (возможность запуска на Linux/Windows)\n",
    "- предусмотреть перенос всех метаданных в CKAN в виде тегов и атрибутов\n",
    "- переносить все данные в CKAN или на отдельный хостинг (например, в Github)\n",
    "## Данные\n",
    "- Персоналии - http://www.patriarchia.ru/db/persons/\n",
    "- Организации - http://www.patriarchia.ru/db/organizations/\n",
    "- Богослужебные указания - http://www.patriarchia.ru/bu/\n",
    "- Храмы - http://map.patriarhia.ru/\n",
    "## Пожелания\n",
    "- реализация в виде программы командной строки или веб-приложения\n",
    "- возможность запуска на MacOSX\n",
    "- использовать скриптовые языки такие как Python, Perl, R и другие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests      \n",
    "import numpy as np   \n",
    "import pandas as pd  \n",
    "import time          \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_link = \"http://www.patriarchia.ru\"\n",
    "link_persons = \"http://www.patriarchia.ru/db/persons/\"\n",
    "link_organizations = \"http://www.patriarchia.ru/db/organizations/\"\n",
    "link_guidance = \"http://www.patriarchia.ru/bu/\"\n",
    "link_churches = \"http://map.patriarhia.ru/\"\n",
    "\n",
    "file_path = os.getcwd() + \"/data/{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "code_folding": [
     2,
     5,
     16,
     48,
     97,
     113,
     148,
     178
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def getSoup(self, link):\n",
    "        \"\"\"\n",
    "            Returns bs4-formatted page\n",
    "        \"\"\"\n",
    "        html = requests.get(link)\n",
    "        if html.status_code == 200:\n",
    "            soup = BeautifulSoup(html.content, 'lxml')\n",
    "        else:\n",
    "            soup = None\n",
    "        return soup\n",
    "    \n",
    "    def getAllLinks(self, dataset_type, url=None):\n",
    "        \"\"\"\n",
    "            Returns a list with names, links and affiliations for persons and organizations\n",
    "            or dates for guidance\n",
    "\n",
    "            dataset_type: string\n",
    "                persons/organizations/guidance\n",
    "                \n",
    "            url: string\n",
    "                url to parse data from\n",
    "        \"\"\"\n",
    "        \n",
    "        if dataset_type=='guidance':\n",
    "            datelist = pd.date_range(\n",
    "                pd.to_datetime(start_date), end=pd.to_datetime(end_date)\n",
    "            ).tolist()\n",
    "            datelist = [date.strftime(\"%Y-%m-%d\") for date in datelist]\n",
    "            return datelist\n",
    "\n",
    "        links = getSoup(url)\n",
    "        links = links.find('div', attrs={\"class\":\"main\"})\n",
    "        links = links.findAll('div', attrs={'class':'news'})\n",
    "\n",
    "        dfLinks = []\n",
    "        for link in links:\n",
    "            name = link.find('h4').text\n",
    "            linkParsed = main_link + link.find('a')['href']\n",
    "            affiliation = link.find('div').text\n",
    "            dfLinks.append({\"name\":name, \"link\":linkParsed, \"affiliation\":affiliation})\n",
    "\n",
    "        return dfLinks\n",
    "\n",
    "    def getPageData(link):\n",
    "        \"\"\"\n",
    "            Returns a dict with parsed field:value pairs from page\n",
    "\n",
    "            link: string\n",
    "                link to page\n",
    "        \"\"\"\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        # use printing version of page \n",
    "        link = link.replace(\"text\", \"print\")\n",
    "\n",
    "        # first - get the page content\n",
    "        pageContent = getSoup(link)\n",
    "\n",
    "        # trying to find any images\n",
    "        try:\n",
    "            image_link = pageContent.find('div', attrs={'class':'news_img'})\n",
    "            image_link = image_link.find('img')['src']\n",
    "            data.update({'image_link':image_link})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # now concentrate on text\n",
    "        pageContent = pageContent.find('div', attrs={'class':'main'})\n",
    "        pageContent = pageContent.findAll(\"dl\")\n",
    "\n",
    "        # second - find all field-value pairs on page\n",
    "        for text in pageContent:        \n",
    "            number_of_fields = len(text.findAll('dt'))*2-1\n",
    "            if number_of_fields>0:\n",
    "                parsedFields = []\n",
    "                currentValue = text.find('dt')\n",
    "                parsedFields.append(' '.join(currentValue.findAll(text=True)).strip().replace(u'\\xa0', u' '))\n",
    "\n",
    "                while number_of_fields>0:\n",
    "                    currentValue = currentValue.nextSibling\n",
    "                    if currentValue != '\\n':\n",
    "                        value = ' '.join(currentValue.findAll(text=True)).strip().replace(u'\\xa0', u' ')\n",
    "                        if value:\n",
    "                            parsedFields.append(value)\n",
    "                            number_of_fields -= 1\n",
    "\n",
    "                text = dict(zip(parsedFields[::2], parsedFields[1::2]))\n",
    "                data.update(text)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def getGuidanceData(date):\n",
    "        \"\"\"\n",
    "            Returns dict with name:text pair for particular day\n",
    "        \"\"\"\n",
    "        link = link_guidance + date + '/print.html'\n",
    "        pageContent = getSoup(link)\n",
    "        pageContent = pageContent.find('div', attrs={'class':'main'})\n",
    "        pageContent = pageContent.findAll('p')\n",
    "\n",
    "        name = pageContent[0].getText().strip()\n",
    "        pageContent = [paragraph.getText() for paragraph in pageContent[1:]]\n",
    "        text = \"\\n\".join(pageContent)\n",
    "        data = {'date':date, 'name':name, 'text':text}\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def saveDataFrame(self, final, dataset_type):\n",
    "        \"\"\"\n",
    "            Save final file into csv and json files\n",
    "            \n",
    "            final: list\n",
    "                parsed data from pages\n",
    "                \n",
    "            dataset_type: string\n",
    "                persons or organizations\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.DataFrame(final)\n",
    "        df.columns = df.columns.str.strip(':')\n",
    "        \n",
    "        if dataset_type==\"persons\":\n",
    "            csv_path = file_path.format(\"persons.csv\")\n",
    "            json_path = file_path.format(\"persons.json\")\n",
    "            column_names = ['name','affiliation','Страна','Дата рождения', \n",
    "                           'Дата пострига','Дата хиротонии', 'Дата смерти',\n",
    "                           'День ангела', 'Епархия', 'Место работы', 'Образование', \n",
    "                           'Биография', 'Награды', 'Научные труды, публикации', \n",
    "                           'E-mail', 'Web-сайт', 'link', 'image_link']\n",
    "            df = df[column_names]\n",
    "        elif dataset_type==\"organizations\":\n",
    "            csv_path = file_path.format(\"organizations.csv\")\n",
    "            json_path = file_path.format(\"organizations.json\")\n",
    "        else:\n",
    "            csv_path = file_path.format(\"guidance.csv\")\n",
    "            json_path = file_path.format(\"guidance.json\")\n",
    "        \n",
    "        df.to_csv(csv_path, encoding='utf8')\n",
    "        df.to_json(json_path)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def runCrawler(self, links_data, dataset_type='other'):\n",
    "        \"\"\"\n",
    "            Iterate over all links in links_data and return final dataset\n",
    "            \n",
    "            links_data: list\n",
    "                list with links to pages\n",
    "            \n",
    "            dataset_type: string\n",
    "                guidance or other\n",
    "        \"\"\"\n",
    "        final = []\n",
    "\n",
    "        for page in tqdm_notebook(links_data):\n",
    "            try:\n",
    "                if dataset_type=='guidance':\n",
    "                    currentDateData = self.getGuidanceData(page)\n",
    "                    final.append(currentDateData)\n",
    "                else:\n",
    "                    data = self.getPageData(page['link'])\n",
    "                    page.update(data)\n",
    "                    final.append(page)\n",
    "                time.sleep(0.5)\n",
    "            except Exception as e:\n",
    "                # print(e)                           # DEBUG\n",
    "                # print('problem',  person['link'])  # DEBUG\n",
    "                time.sleep(30)\n",
    "                \n",
    "        return final\n",
    "        \n",
    "    \n",
    "    def runParsing(self, dataset_type):\n",
    "        \n",
    "        if dataset_type=='persons':\n",
    "            link = link_persons\n",
    "        elif dataset_type=='organizations':\n",
    "            link = link_organizations\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        allLinks = self.getAllLinks(dataset_type, link)\n",
    "        data = self.runCrawler(allLinks, dataset_type)\n",
    "        self.saveDataFrame(data, dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfa898619ba44c0bbb450b93828b193"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parser = Parser()\n",
    "parser.runParsing('persons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "code_folding": [
     0,
     11,
     32
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSoup(link):\n",
    "    \"\"\"\n",
    "        Returns bs4-formatted page\n",
    "    \"\"\"\n",
    "    html = requests.get(link)\n",
    "    if html.status_code == 200:\n",
    "        soup = BeautifulSoup(html.content, 'lxml')\n",
    "    else:\n",
    "        soup = None\n",
    "    return soup\n",
    "\n",
    "def getAllLinks(url):\n",
    "    \"\"\"\n",
    "        Returns a list with names, links and affiliations    \n",
    "        \n",
    "        url: string\n",
    "            url to parse data from\n",
    "    \"\"\"\n",
    "    \n",
    "    links = getSoup(url)\n",
    "    links = links.find('div', attrs={\"class\":\"main\"})\n",
    "    links = links.findAll('div', attrs={'class':'news'})\n",
    "    \n",
    "    dfLinks = []\n",
    "    for link in links:\n",
    "        name = link.find('h4').text\n",
    "        linkParsed = main_link + link.find('a')['href']\n",
    "        affiliation = link.find('div').text\n",
    "        dfLinks.append({\"name\":name, \"link\":linkParsed, \"affiliation\":affiliation})\n",
    "        \n",
    "    return dfLinks\n",
    "\n",
    "def getPageData(link):\n",
    "    \"\"\"\n",
    "        Returns a dict with parsed field:value pairs from page\n",
    "    \n",
    "        link: string\n",
    "            link to page\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # use printing version of page \n",
    "    if 'text' in link:\n",
    "        link = link.replace(\"text\", \"print\")\n",
    "    \n",
    "    # first - get the page content\n",
    "    pageContent = getSoup(link)\n",
    "    \n",
    "    # trying to find any images\n",
    "    try:\n",
    "        image_link = pageContent.find('div', attrs={'class':'news_img'})\n",
    "        image_link = image_link.find('img')['src']\n",
    "        data.update({'image_link':image_link})\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # now concentrate on text\n",
    "    pageContent = pageContent.find('div', attrs={'class':'main'})\n",
    "    pageContent = pageContent.findAll(\"dl\")\n",
    "\n",
    "    # second - find all field-value pairs on page\n",
    "    for text in pageContent:        \n",
    "        number_of_fields = len(text.findAll('dt'))*2-1\n",
    "        if number_of_fields>0:\n",
    "            parsedFields = []\n",
    "            currentValue = text.find('dt')\n",
    "            parsedFields.append(' '.join(currentValue.findAll(text=True)).strip().replace(u'\\xa0', u' '))\n",
    "\n",
    "            while number_of_fields>0:\n",
    "                currentValue = currentValue.nextSibling\n",
    "                if currentValue != '\\n':\n",
    "                    value = ' '.join(currentValue.findAll(text=True)).strip().replace(u'\\xa0', u' ')\n",
    "                    if value:\n",
    "                        parsedFields.append(value)\n",
    "                        number_of_fields -= 1\n",
    "\n",
    "            text = dict(zip(parsedFields[::2], parsedFields[1::2]))\n",
    "            data.update(text)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9d4d751f29491aacaefea47cbd1c6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = []\n",
    "for page in tqdm_notebook(dfOrganizations):\n",
    "    try:\n",
    "        data = getPageData(page['link'])\n",
    "        page.update(data)\n",
    "        final.append(page)\n",
    "        time.sleep(0.6)\n",
    "    except Exception as e:\n",
    "        # print(e)                           # DEBUG\n",
    "        # print('problem',  person['link'])  # DEBUG\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_date = '2009-01-14'\n",
    "end_date = '2019-01-13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aab705e94e642cdad17660996a5dde9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'find'\n",
      "problem http://www.patriarchia.ru/db/text/80876.html\n",
      "'NoneType' object has no attribute 'find'\n",
      "problem http://www.patriarchia.ru/db/text/80876.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = []\n",
    "for date in tqdm_notebook(datelist):\n",
    "    try:\n",
    "        currentDateData = getGuidanceData(date)\n",
    "        final.append(currentDateData)\n",
    "        time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(e)                           # DEBUG\n",
    "        print('problem',  date)  # DEBUG\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Среда. Обре́зание Господне. Свт. Василия Велик...</td>\n",
       "      <td>Примечание. В Типиконе сказано: «А́ще храм свя...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Четверг. Предпразднство Богоявления. Прп. Сера...</td>\n",
       "      <td>Свт. Сильве́стра, папы Римского. Прав. Иулиани...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Пятница. Предпразднство Богоявления. Прор. Мал...</td>\n",
       "      <td>Служба прор. Малахии и мч. Гордия не имеет пра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Суббота пред Богоявлением. Предпразднство Бого...</td>\n",
       "      <td>Свт. Евста́фия, архиеп. Сербского.\\nПрп. Ахи́л...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Неделя 31-я по Пятидесятнице, пред Богоявление...</td>\n",
       "      <td>Прп. Синклитики́и Александрийской.\\nВоскресная...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Среда. Обре́зание Господне. Свт. Василия Велик...   \n",
       "1  Четверг. Предпразднство Богоявления. Прп. Сера...   \n",
       "2  Пятница. Предпразднство Богоявления. Прор. Мал...   \n",
       "3  Суббота пред Богоявлением. Предпразднство Бого...   \n",
       "4  Неделя 31-я по Пятидесятнице, пред Богоявление...   \n",
       "\n",
       "                                                text  \n",
       "0  Примечание. В Типиконе сказано: «А́ще храм свя...  \n",
       "1  Свт. Сильве́стра, папы Римского. Прав. Иулиани...  \n",
       "2  Служба прор. Малахии и мч. Гордия не имеет пра...  \n",
       "3  Свт. Евста́фия, архиеп. Сербского.\\nПрп. Ахи́л...  \n",
       "4  Прп. Синклитики́и Александрийской.\\nВоскресная...  "
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/guidance.csv', encoding='utf8')\n",
    "df.to_json('data/guidance.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGuidanceData(date):\n",
    "    \"\"\"\n",
    "        Returns dict with name:text pair for particular day\n",
    "    \"\"\"\n",
    "    link = link_guidance + date + '/print.html'\n",
    "    pageContent = getSoup(link)\n",
    "    pageContent = pageContent.find('div', attrs={'class':'main'})\n",
    "    pageContent = pageContent.findAll('p')\n",
    "    \n",
    "    name = pageContent[0].getText().strip()\n",
    "    pageContent = [paragraph.getText() for paragraph in pageContent[1:]]\n",
    "    text = \"\\n\".join(pageContent)\n",
    "    data = {'date':date, 'name':name, 'text':text}\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "326px",
    "left": "939.091px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
